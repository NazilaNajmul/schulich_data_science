{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "file_path = \"c:/Users/nazil/Downloads/data_set_hackathon.csv\"  # Update with the correct path\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'order_date' to datetime\n",
    "df['order_date'] = pd.to_datetime(df['order_date'], format='%d.%m.%Y')\n",
    "\n",
    "# Extract 'order_month' (year and month) from 'order_date'\n",
    "df['order_month'] = df['order_date'].dt.to_period('M')\n",
    "\n",
    "# Group by 'order_month' and count distinct 'Customer Order Code'\n",
    "monthly_orders = df.groupby('order_month').agg(\n",
    "    Distinct_Orders=('Customer Order Code', 'nunique')\n",
    ").reset_index()\n",
    "\n",
    "# Display the distinct orders received in each month\n",
    "print(\"Distinct Orders Received in Each Month:\")\n",
    "print(monthly_orders)\n",
    "\n",
    "# Add seasonal features if needed (for example, you can create seasonal flags like Winter, Spring, etc.)\n",
    "# Here, just creating dummy variables as an example:\n",
    "monthly_orders['Season_Winter'] = (monthly_orders['order_month'].dt.month == 12) | (monthly_orders['order_month'].dt.month == 1) | (monthly_orders['order_month'].dt.month == 2)\n",
    "monthly_orders['Season_Spring'] = (monthly_orders['order_month'].dt.month == 3) | (monthly_orders['order_month'].dt.month == 4) | (monthly_orders['order_month'].dt.month == 5)\n",
    "monthly_orders['Season_Summer'] = (monthly_orders['order_month'].dt.month == 6) | (monthly_orders['order_month'].dt.month == 7) | (monthly_orders['order_month'].dt.month == 8)\n",
    "\n",
    "# Log transformation to stabilize variance\n",
    "monthly_orders['Log Distinct Orders'] = np.log1p(monthly_orders['Distinct_Orders'])\n",
    "\n",
    "# Split data into train and test (80% train, 20% test)\n",
    "train_size = int(len(monthly_orders) * 0.8)\n",
    "train_data_log = monthly_orders['Log Distinct Orders'][:train_size]\n",
    "test_data_log = monthly_orders['Log Distinct Orders'][train_size:]\n",
    "\n",
    "# Exogenous variables (e.g., seasonality features)\n",
    "exog_train = monthly_orders[['Season_Winter', 'Season_Spring', 'Season_Summer']][:train_size]\n",
    "exog_test = monthly_orders[['Season_Winter', 'Season_Spring', 'Season_Summer']][train_size:]\n",
    "\n",
    "# Fit SARIMAX model\n",
    "sarimax_model_log = SARIMAX(\n",
    "    train_data_log,\n",
    "    exog=exog_train,\n",
    "    order=(1, 0, 0),  # SARIMAX order (p, d, q)\n",
    "    enforce_stationarity=False,\n",
    "    enforce_invertibility=False\n",
    ")\n",
    "sarimax_results_log = sarimax_model_log.fit(disp=False)\n",
    "\n",
    "# Forecast the next 5 months\n",
    "forecast_steps = 5\n",
    "forecast_log = sarimax_results_log.get_forecast(steps=forecast_steps, exog=exog_test.iloc[:forecast_steps])\n",
    "forecast_mean_log = forecast_log.predicted_mean\n",
    "forecast_ci_log = forecast_log.conf_int()\n",
    "\n",
    "# Transform forecast back from log scale\n",
    "forecast_mean = np.expm1(forecast_mean_log)\n",
    "forecast_ci = np.expm1(forecast_ci_log)\n",
    "\n",
    "# Calculate MAPE (Mean Absolute Percentage Error)\n",
    "mape_sarimax_log = mean_absolute_percentage_error(np.expm1(test_data_log[:forecast_steps]), forecast_mean)\n",
    "print(f\"SARIMAX Model with Log Transformation - Mean Absolute Percentage Error (MAPE): {mape_sarimax_log * 100:.2f}%\")\n",
    "\n",
    "# Output the forecasted values for future months\n",
    "forecast_list = list(forecast_mean)\n",
    "print(f\"Forecast for the next 5 months: {forecast_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Convert 'items' column to integer (ensure proper numeric conversion)\n",
    "df['items'] = pd.to_numeric(df['items'], errors='coerce')\n",
    "\n",
    "# Set a threshold for determining if an order is 'demanded' based on 'items'\n",
    "threshold = 5  # Modify this threshold based on your needs\n",
    "df['demanded'] = (df['items'] > threshold).astype(int)  # Mark as 1 if items > threshold, else 0\n",
    "\n",
    "# Create seasonal features (using map and one-hot encoding)\n",
    "df['Season'] = df['order_date'].dt.month.map(lambda x: 'Winter' if x in [12, 1, 2] else\n",
    "                                                     'Spring' if x in [3, 4, 5] else\n",
    "                                                     'Summer' if x in [6, 7, 8] else 'Autumn')\n",
    "\n",
    "# One-hot encode the 'Season' categorical variable\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_season = encoder.fit_transform(df[['Season']])\n",
    "season_encoded_df = pd.DataFrame(encoded_season, columns=encoder.get_feature_names_out(['Season']))\n",
    "\n",
    "# Ensure all potential seasons are in the column names (add missing columns manually if necessary)\n",
    "for season in ['Season_Winter', 'Season_Spring', 'Season_Summer', 'Season_Autumn']:\n",
    "    if season not in season_encoded_df.columns:\n",
    "        season_encoded_df[season] = 0\n",
    "\n",
    "# Encode 'Product Code' using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "df['Product Code_encoded'] = label_encoder.fit_transform(df['Product Code'])\n",
    "\n",
    "# Ensure categorical columns are of type 'category'\n",
    "df['Customer Country Code'] = df['Customer Country Code'].astype('category')\n",
    "df['Route'] = df['Route'].astype('category')\n",
    "\n",
    "# Define features and target variable for the classification model\n",
    "categorical_features = ['Customer Country Code', 'Route']  # These are categorical columns\n",
    "numerical_features = ['value', 'items'] + list(season_encoded_df.columns) + ['Product Code_encoded']\n",
    "\n",
    "# Define target variable (Product Code or Demand Count based on project objective)\n",
    "y = df['demanded']  # If you're predicting demand (binary classification)\n",
    "# y = df['Product Code']  # Uncomment if you're predicting the specific product code (multi-class classification)\n",
    "\n",
    "# Define X by combining the features\n",
    "X = df[categorical_features + numerical_features]\n",
    "\n",
    "# Split the dataset into train and test sets (80% for training and 20% for testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing - Handle missing values and one-hot encode categorical variables using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "        ('num', SimpleImputer(strategy='mean'), numerical_features)  # Impute missing values in numerical features\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep the rest of the features as they are\n",
    ")\n",
    "\n",
    "# Define the RandomForestClassifier with class_weight='balanced' to handle any imbalance\n",
    "classifier = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "\n",
    "# Create a pipeline for preprocessing and model training\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', classifier)\n",
    "])\n",
    "\n",
    "# Step 1: Fit the model without hyperparameter tuning (initial model)\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 2: Evaluate the model on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Step 3: Print classification report to evaluate the model\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 4: Calculate and print accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy Score: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Step 5: Print confusion matrix to analyze misclassifications\n",
    "print(\"Random Forest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Step 6: Hyperparameter Tuning (Optional)\n",
    "# Perform Grid Search for hyperparameter tuning if needed\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'classifier__max_depth': [10, 20, None],       # Maximum depth of the tree\n",
    "    'classifier__min_samples_split': [2, 5, 10],   # Minimum number of samples required to split an internal node\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],     # Minimum number of samples required at each leaf node\n",
    "    'classifier__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider at each split\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters from Grid Search:\")\n",
    "print(best_params)\n",
    "\n",
    "# Train the best model from the grid search\n",
    "best_pipeline = grid_search.best_estimator_\n",
    "\n",
    "# Step 7: Evaluate the best model from the grid search\n",
    "y_pred_best = best_pipeline.predict(X_test)\n",
    "\n",
    "# Print classification report for the tuned model\n",
    "print(\"Best Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_best))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "feature_importances = best_rf_model.feature_importances_\n",
    "features = X_train.columns\n",
    "plt.barh(features, feature_importances)\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
