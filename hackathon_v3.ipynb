{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv(\"c:/Users/nazil/Downloads/data_set_hackathon.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values after imputation:\n",
      " Customer Order Code    0\n",
      "Season                 0\n",
      "Spring                 0\n",
      "Summer                 0\n",
      "Winter                 0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazil\\AppData\\Local\\Temp\\ipykernel_21792\\96681104.py:2: UserWarning: Parsing dates in %d.%m.%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data['order_date'] = pd.to_datetime(data['order_date'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['Autumn'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 49\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing values after imputation:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, monthly_orders_df_imputed\u001b[38;5;241m.\u001b[39misnull()\u001b[38;5;241m.\u001b[39msum())\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Step 5: Split the data for training and testing\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mmonthly_orders_df_imputed\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSpring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSummer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAutumn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Features: Season dummies\u001b[39;00m\n\u001b[0;32m     50\u001b[0m y \u001b[38;5;241m=\u001b[39m monthly_orders_df_imputed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCustomer Order Code\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Target: Number of distinct orders\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Train-test split\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nazil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\nazil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nazil\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Autumn'] not in index\""
     ]
    }
   ],
   "source": [
    "# Step 1: Prepare data\n",
    "data['order_date'] = pd.to_datetime(data['order_date'])\n",
    "data['Year-Month'] = data['order_date'].dt.to_period('M')\n",
    "\n",
    "# Aggregate by month and count distinct order codes\n",
    "monthly_orders = data.groupby('Year-Month')['Customer Order Code'].nunique()\n",
    "\n",
    "# Step 2: Create seasonal dummy variables (including Winter as the reference category)\n",
    "data['Season'] = data['order_date'].dt.month.map(lambda x: 'Winter' if x in [12, 1, 2] else\n",
    "                                                 'Spring' if x in [3, 4, 5] else\n",
    "                                                 'Summer' if x in [6, 7, 8] else 'Autumn')\n",
    "\n",
    "# Create dummy variables for seasons (drop Winter as reference)\n",
    "season_dummies = pd.get_dummies(data['Season'], drop_first=True)\n",
    "\n",
    "# Merge seasonal data with the monthly orders\n",
    "monthly_orders_df = pd.DataFrame(monthly_orders)\n",
    "monthly_orders_df['Season'] = data.groupby('Year-Month')['Season'].first().values\n",
    "monthly_orders_df = pd.concat([monthly_orders_df, season_dummies], axis=1)\n",
    "\n",
    "# Step 3: Handle missing values using SimpleImputer (separate strategy for numerical and categorical features)\n",
    "# Numeric columns (for imputation with mean)\n",
    "numeric_features = monthly_orders_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Categorical columns (for imputation with most_frequent)\n",
    "categorical_features = monthly_orders_df.select_dtypes(include=[object]).columns\n",
    "\n",
    "# Create transformers: Use 'mean' for numeric columns and 'most_frequent' for categorical columns\n",
    "numeric_transformer = SimpleImputer(strategy='mean')\n",
    "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
    "\n",
    "# Use ColumnTransformer to apply the transformers to appropriate columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Apply the transformers to the data\n",
    "monthly_orders_df_imputed = preprocessor.fit_transform(monthly_orders_df)\n",
    "\n",
    "# Convert the imputed data back into a DataFrame\n",
    "monthly_orders_df_imputed = pd.DataFrame(monthly_orders_df_imputed, columns=monthly_orders_df.columns)\n",
    "\n",
    "# Step 4: Check for missing values after imputation\n",
    "print(\"Missing values after imputation:\\n\", monthly_orders_df_imputed.isnull().sum())\n",
    "\n",
    "# Step 5: Split the data for training and testing\n",
    "X = monthly_orders_df_imputed[['Spring', 'Summer', 'Autumn']]  # Features: Season dummies\n",
    "y = monthly_orders_df_imputed['Customer Order Code']  # Target: Number of distinct orders\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 7: Forecast\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 8: Evaluate Model using MAPE\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f\"MAPE for Monthly Orders Forecasting: {mape:.2f}%\")\n",
    "\n",
    "# Step 9: Plot Actual vs Predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(y_test.index, y_test, label='Actual Orders', color='blue')\n",
    "plt.plot(y_test.index, y_pred, label='Predicted Orders', color='red')\n",
    "plt.title('Forecasting Monthly Orders using Linear Regression')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare data for classification\n",
    "data['Season'] = data['order_date'].dt.month.map(lambda x: 'Winter' if x in [12, 1, 2] else\n",
    "                                                 'Spring' if x in [3, 4, 5] else\n",
    "                                                 'Summer' if x in [6, 7, 8] else 'Autumn')\n",
    "\n",
    "# One-hot encoding for season and other categorical features\n",
    "season_dummies = pd.get_dummies(data['Season'], drop_first=True)\n",
    "data = pd.concat([data, season_dummies], axis=1)\n",
    "\n",
    "# Define features and target variable\n",
    "X = data[['value', 'Spring', 'Summer', 'Autumn', 'Customer Order Code']]  # Include relevant features\n",
    "y = data['Product Code']  # Target: Product Code\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# Step 1: Quantile Transformation for Demand Quantities\n",
    "quantile_transformer = QuantileTransformer(n_quantiles=4, output_distribution='uniform')\n",
    "quantile_transformed = quantile_transformer.fit_transform(data[['items']])\n",
    "\n",
    "# Add quantile categories to the dataset\n",
    "data['Quantity_Quantile'] = pd.cut(quantile_transformed, bins=4, labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "\n",
    "# Display the quantile classification for quantities\n",
    "print(data[['items', 'Quantity_Quantile']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Quantile Transformation for Lead Times\n",
    "lead_time_data = (data['requested_delivery_date'] - data['order_date']).dt.days\n",
    "lead_time_quantile_transformer = QuantileTransformer(n_quantiles=4, output_distribution='uniform')\n",
    "lead_time_quantiles = lead_time_quantile_transformer.fit_transform(lead_time_data.values.reshape(-1, 1))\n",
    "\n",
    "# Add quantile categories to the dataset\n",
    "data['LeadTime_Quantile'] = pd.cut(lead_time_quantiles, bins=4, labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "\n",
    "# Display the quantile classification for lead times\n",
    "print(data[['requested_delivery_date', 'order_date', 'LeadTime_Quantile']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define a function to simulate demand based on uncertainties\n",
    "def monte_carlo_simulation(data, n_simulations=1000, lead_time_horizon=5):\n",
    "    simulated_demand = []\n",
    "\n",
    "    for _ in range(n_simulations):\n",
    "        # Randomly sample product demand categories, quantity quantiles, and lead times\n",
    "        product_sample = data['Product Code'].sample()\n",
    "        quantity_sample = data['Quantity_Quantile'].sample()\n",
    "        lead_time_sample = data['LeadTime_Quantile'].sample()\n",
    "\n",
    "        # Simulate demand based on these categories (we can use random values or predefined distributions)\n",
    "        simulated_order_quantity = np.random.choice([1, 2, 3], p=[0.3, 0.4, 0.3])  # example distribution\n",
    "        simulated_lead_time = np.random.choice(['Short', 'Medium', 'Long', 'Very Long'], p=[0.4, 0.3, 0.2, 0.1])\n",
    "\n",
    "        # Simulate demand over the next 5 months for the selected product\n",
    "        demand_for_product = simulated_order_quantity * lead_time_horizon\n",
    "        simulated_demand.append(demand_for_product)\n",
    "\n",
    "    # Step 2: Return the average demand across simulations\n",
    "    return np.mean(simulated_demand)\n",
    "\n",
    "# Step 3: Simulate for each product\n",
    "product_demand_simulation = {}\n",
    "for product in data['Product Code'].unique():\n",
    "    product_data = data[data['Product Code'] == product]\n",
    "    product_demand_simulation[product] = monte_carlo_simulation(product_data)\n",
    "\n",
    "# Step 4: Display simulated demand for each product\n",
    "for product, demand in product_demand_simulation.items():\n",
    "    print(f\"Product {product} - Simulated Demand: {demand:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming y_test and predicted values are available\n",
    "mape_5_month = mean_absolute_percentage_error(y_test_5_month, predicted_5_month)\n",
    "mape_2_month = mean_absolute_percentage_error(y_test_2_month, predicted_2_month)\n",
    "\n",
    "print(f\"MAPE for 5-Month Lead Time: {mape_5_month:.2f}%\")\n",
    "print(f\"MAPE for 2-Month Lead Time: {mape_2_month:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "data = pd.read_csv(\"c:/Users/nazil/Downloads/data_set_hackathon.csv\")\n",
    "data['order_date'] = pd.to_datetime(data['order_date'])\n",
    "data['requested_delivery_date'] = pd.to_datetime(data['requested_delivery_date'])\n",
    "data['Year-Month'] = data['order_date'].dt.to_period('M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Data by Monthly Orders\n",
    "monthly_orders = data.groupby('Year-Month')['Customer Order Code'].nunique()\n",
    "\n",
    "# Split Data into Train/Test\n",
    "train = monthly_orders[:int(len(monthly_orders)*0.8)]\n",
    "test = monthly_orders[int(len(monthly_orders)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time Series Analysis and SARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the maximum allowed lags\n",
    "max_lags = len(monthly_orders) // 2\n",
    "\n",
    "# ACF Plot\n",
    "plot_acf(monthly_orders, lags=max_lags)\n",
    "plt.title(\"ACF of Monthly Orders\")\n",
    "plt.show()\n",
    "\n",
    "# PACF Plot\n",
    "plot_pacf(monthly_orders, lags=max_lags)\n",
    "plt.title(\"PACF of Monthly Orders\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA Model Fitting\n",
    "model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(0, 1, 1, 12))\n",
    "model_fit = model.fit(disp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Diagnostics\n",
    "residuals = model_fit.resid\n",
    "\n",
    "# Convert PeriodIndex to DatetimeIndex for plotting\n",
    "residuals.index = residuals.index.to_timestamp()\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(residuals, label='Residuals', color='blue')\n",
    "plt.axhline(0, linestyle='--', color='red', label='Zero Line')\n",
    "plt.title(\"SARIMA Model Residuals\")\n",
    "plt.xlabel(\"Time\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Test for White Noise\n",
    "ljung_test = acorr_ljungbox(residuals, lags=[10], return_df=True)\n",
    "print(\"Ljung-Box Test Results:\\n\", ljung_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast and Evaluation\n",
    "forecast = model_fit.get_forecast(steps=len(test))\n",
    "forecast_values = forecast.predicted_mean\n",
    "forecast_ci = forecast.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAPE Calculation for SARIMA\n",
    "mape = mean_absolute_percentage_error(test.values, forecast_values)\n",
    "print(f\"SARIMA MAPE: {mape:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert PeriodIndex to DatetimeIndex for plotting\n",
    "monthly_orders.index = monthly_orders.index.to_timestamp()\n",
    "forecast_ci.index = forecast_ci.index.to_timestamp()\n",
    "\n",
    "# Plot Forecast\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(monthly_orders.index, monthly_orders, label='Historical Orders', color='blue')\n",
    "plt.plot(forecast_values.index, forecast_values, label='Forecasted Orders', color='red')\n",
    "plt.fill_between(forecast_ci.index, forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1], color='red', alpha=0.2)\n",
    "plt.title(\"SARIMA Forecast with 95% Confidence Intervals\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Number of Distinct Orders\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print forecast values and confidence intervals\n",
    "print(\"Forecasted Values:\")\n",
    "print(forecast.predicted_mean)\n",
    "print(\"\\nConfidence Intervals:\")\n",
    "print(forecast_ci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Model for Product Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empirical Quantiles for Demand and Lead Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import mquantiles\n",
    "\n",
    "# Empirical Quantiles for Quantity\n",
    "quantile_25, quantile_50, quantile_75 = mquantiles(data['items'].astype(float), prob=[0.25, 0.5, 0.75])\n",
    "print(f\"Demand Quantiles - 25%: {quantile_25}, 50%: {quantile_50}, 75%: {quantile_75}\")\n",
    "\n",
    "# Empirical Quantiles for Lead Time\n",
    "lead_time = (data['requested_delivery_date'] - data['order_date']).dt.days\n",
    "lead_time_quantiles = mquantiles(lead_time, prob=[0.25, 0.5, 0.75])\n",
    "print(f\"Lead Time Quantiles - 25%: {lead_time_quantiles[0]}, 50%: {lead_time_quantiles[1]}, 75%: {lead_time_quantiles[2]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monte Carlo Simulation for Total Demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_simulations = 1000\n",
    "simulated_total_demand_5_month = []\n",
    "\n",
    "for _ in range(n_simulations):\n",
    "    simulated_monthly_orders = np.random.poisson(train.mean())\n",
    "    simulated_quantity = np.random.choice([quantile_25, quantile_50, quantile_75])\n",
    "    simulated_lead_time = np.random.choice(lead_time_quantiles)\n",
    "    \n",
    "    total_demand = simulated_monthly_orders * simulated_quantity * simulated_lead_time\n",
    "    simulated_total_demand_5_month.append(total_demand)\n",
    "\n",
    "# Analyze Simulation Results\n",
    "simulated_total_demand_5_month = np.array(simulated_total_demand_5_month)\n",
    "mean_demand = simulated_total_demand_5_month.mean()\n",
    "ci_5_month = np.percentile(simulated_total_demand_5_month, [2.5, 97.5])\n",
    "\n",
    "print(f\"Simulated 5-Month Demand: Mean={mean_demand}, 95% CI={ci_5_month}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
